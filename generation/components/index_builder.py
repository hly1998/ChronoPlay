"""ç´¢å¼•æ„å»ºå™¨ - è´Ÿè´£ä¸ºåˆ†æ®µæ„å»ºå’Œç®¡ç†å‘é‡ç´¢å¼•"""

import json
from pathlib import Path
from typing import List, Dict, Any
from llama_index.core import (
    VectorStoreIndex,
    Document,
    StorageContext,
    load_index_from_storage
)
from llama_index.core.retrievers import VectorIndexRetriever


class IndexBuilder:
    """ç´¢å¼•æ„å»ºå™¨ç±»"""

    def __init__(self, corpus_path: Path, output_dir: Path, similarity_top_k: int = 3):
        """
        åˆå§‹åŒ–ç´¢å¼•æ„å»ºå™¨

        Args:
            corpus_path: è¯­æ–™åº“è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            similarity_top_k: æ£€ç´¢æ—¶è¿”å›çš„ç›¸ä¼¼æ–‡æ¡£æ•°é‡
        """
        self.corpus_path = corpus_path
        self.output_dir = output_dir
        self.similarity_top_k = similarity_top_k
        self.index = None
        self.retriever = None

    def load_corpus_for_segment(self, segment_id: int) -> List[Document]:
        """
        ä¸ºæŒ‡å®šåˆ†æ®µåŠ è½½è¯­æ–™åº“æ•°æ®

        Args:
            segment_id: åˆ†æ®µID

        Returns:
            Documentå¯¹è±¡åˆ—è¡¨
        """
        documents = []

        # åŠ è½½timelessæ•°æ®
        timeless_corpus = self.corpus_path / "segment_timeless" / "corpus.jsonl"
        if timeless_corpus.exists():
            documents.extend(self._load_corpus_from_file(timeless_corpus, "timeless"))
            print(f"ğŸ“š åŠ è½½timelessæ•°æ®: {len(documents)} ä¸ªæ–‡æ¡£")

        # åŠ è½½æŒ‡å®šåˆ†æ®µæ•°æ®
        segment_corpus = self.corpus_path / f"segment_{segment_id}" / "corpus.jsonl"
        if segment_corpus.exists():
            segment_docs = self._load_corpus_from_file(segment_corpus, f"segment_{segment_id}")
            documents.extend(segment_docs)
            print(f"ğŸ“š åŠ è½½åˆ†æ®µ{segment_id}æ•°æ®: {len(segment_docs)} ä¸ªæ–‡æ¡£")

        print(f"ğŸ“š åˆ†æ®µ {segment_id} è¯­æ–™åŠ è½½å®Œæˆ: æ€»è®¡ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents

    def _load_corpus_from_file(self, file_path: Path, source: str) -> List[Document]:
        """ä»æ–‡ä»¶åŠ è½½è¯­æ–™åº“æ•°æ®"""
        documents = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        data = json.loads(line.strip())

                        # æ„å»ºåŸºç¡€metadata
                        processed_metadata = {
                            'id': data.get('id', f'{source}_doc_{line_num}'),
                            'source': source,
                        }

                        # å¤„ç†corpusæ–‡ä»¶ä¸­çš„entitieså­—æ®µï¼ˆç›´æ¥åœ¨æ ¹çº§åˆ«ï¼‰
                        if 'entities' in data and isinstance(data['entities'], list):
                            # corpusæ–‡ä»¶ä¸­çš„entitiesæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç›´æ¥ä½œä¸ºentity_texts
                            entity_texts = [entity.strip() for entity in data['entities'] if entity and entity.strip()]
                            processed_metadata['entity_texts'] = entity_texts

                        # å¤åˆ¶å…¶ä»–æœ‰ç”¨çš„å­—æ®µåˆ°metadata
                        for key in ['title', 'segment_id', 'extracted_date']:
                            if key in data:
                                processed_metadata[key] = data[key]

                        # å¦‚æœå­˜åœ¨metadataå­—æ®µï¼Œä¹Ÿå¤åˆ¶è¿›æ¥
                        if 'metadata' in data and isinstance(data['metadata'], dict):
                            for key, value in data['metadata'].items():
                                processed_metadata[key] = value

                        doc = Document(
                            text=data.get('contents', data.get('text', '')),
                            metadata=processed_metadata
                        )
                        doc.id_ = data.get('id', f'{source}_doc_{line_num}')
                        documents.append(doc)
                    except json.JSONDecodeError as e:
                        print(f"ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                        continue
        return documents

    def build_segment_index(self, segment_id: int, force_rebuild: bool = False) -> bool:
        """
        ä¸ºæŒ‡å®šåˆ†æ®µæ„å»ºç´¢å¼•

        Args:
            segment_id: åˆ†æ®µID
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print(f"=== ä¸ºåˆ†æ®µ {segment_id} æ„å»ºç´¢å¼• ===")

        # è®¾ç½®åˆ†æ®µä¸“ç”¨çš„ç´¢å¼•ç›®å½•
        segment_index_dir = self.output_dir / f"segment_{segment_id}_index"

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•
        if force_rebuild and segment_index_dir.exists():
            print("ğŸ”„ å¼ºåˆ¶é‡å»ºæ¨¡å¼ï¼šåˆ é™¤ç°æœ‰ç´¢å¼•ç›®å½•...")
            import shutil
            shutil.rmtree(segment_index_dir)

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç´¢å¼•
        if not force_rebuild and segment_index_dir.exists():
            try:
                print(f"æ­£åœ¨åŠ è½½åˆ†æ®µ {segment_id} çš„å·²æœ‰ç´¢å¼•...")
                storage_context = StorageContext.from_defaults(
                    persist_dir=str(segment_index_dir))
                self.index = load_index_from_storage(storage_context)
                print(f"âœ… åˆ†æ®µ {segment_id} ç´¢å¼•åŠ è½½æˆåŠŸ")

                # åˆ›å»ºæ£€ç´¢å™¨
                self.retriever = VectorIndexRetriever(
                    index=self.index,
                    similarity_top_k=self.similarity_top_k
                )
                return True

            except Exception as e:
                print(f"åŠ è½½åˆ†æ®µ {segment_id} ç´¢å¼•å¤±è´¥: {e}, å°†é‡æ–°æ„å»ºç´¢å¼•")

        # åŠ è½½è¯­æ–™åº“æ•°æ®
        documents = self.load_corpus_for_segment(segment_id)

        if not documents:
            print(f"âŒ åˆ†æ®µ {segment_id} æ²¡æœ‰è¯­æ–™åº“æ•°æ®")
            return False

        try:
            # æ„å»ºå‘é‡ç´¢å¼•
            print(f"æ­£åœ¨ä¸ºåˆ†æ®µ {segment_id} æ„å»ºæ–°ç´¢å¼•...")
            self.index = VectorStoreIndex.from_documents(
                documents,
                show_progress=True
            )

            # ä¿å­˜ç´¢å¼•
            self.index.storage_context.persist(persist_dir=str(segment_index_dir))
            print(f"âœ… åˆ†æ®µ {segment_id} ç´¢å¼•æ„å»ºå®Œæˆï¼Œä¿å­˜åˆ°: {segment_index_dir}")

            # åˆ›å»ºæ£€ç´¢å™¨
            self.retriever = VectorIndexRetriever(
                index=self.index,
                similarity_top_k=self.similarity_top_k
            )

            return True

        except Exception as e:
            print(f"âŒ æ„å»ºåˆ†æ®µ {segment_id} ç´¢å¼•å¤±è´¥: {e}")
            return False

    def retrieve_documents(self, query: str) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.retriever:
            raise ValueError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ„å»ºç´¢å¼•")

        nodes = self.retriever.retrieve(query)
        documents = []
        for node in nodes:
            doc_data = {
                'id': node.id_,
                'content': node.get_content(),
                'metadata': node.metadata,
                'score': getattr(node, 'score', 0.0)
            }
            documents.append(doc_data)

        return documents

    def cleanup_index(self, segment_id: int):
        """æ¸…ç†ç´¢å¼•æ–‡ä»¶"""
        segment_index_dir = self.output_dir / f"segment_{segment_id}_index"
        if segment_index_dir.exists():
            import shutil
            shutil.rmtree(segment_index_dir)
            print(f"âœ… åˆ é™¤ç´¢å¼•ç›®å½•: {segment_index_dir}")
