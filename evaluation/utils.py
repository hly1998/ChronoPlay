# -*- coding: utf-8 -*-
"""
ChronoPlayè¯„ä¼°å·¥å…·å‡½æ•°
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
from collections import defaultdict


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_results(results: Dict[str, Any], output_path: str):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    output_dir = os.path.dirname(output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)


def load_jsonl_file(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ ¼å¼æ–‡ä»¶"""
    data = []

    if not os.path.exists(file_path):
        return data

    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                try:
                    data.append(json.loads(line.strip()))
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ {file_path}ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                    continue

    return data


def save_jsonl_file(data: List[Dict[str, Any]], file_path: str):
    """ä¿å­˜JSONLæ ¼å¼æ–‡ä»¶"""
    output_dir = os.path.dirname(file_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')


def get_available_segments(game_name: str, data_dir: str) -> List[int]:
    """è·å–å¯ç”¨çš„åˆ†æ®µIDåˆ—è¡¨"""
    segments = []
    # æ–°è·¯å¾„: data/{æ¸¸æˆåç§°}/segments/segment_{id}
    data_path = Path(data_dir) / game_name / "segments"

    if not data_path.exists():
        return segments

    for item in data_path.iterdir():
        if item.is_dir() and item.name.startswith('segment_'):
            try:
                segment_id = int(item.name.split('_')[1])
                segments.append(segment_id)
            except (ValueError, IndexError):
                continue

    return sorted(segments)


def calculate_metrics_statistics(results: Dict[str, Any]) -> Dict[str, Any]:
    """è®¡ç®—æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯"""
    stats = {}

    for category, metrics in results.items():
        if isinstance(metrics, dict):
            category_stats = {}

            for metric, values in metrics.items():
                if isinstance(values, dict) and 'mean' in values:
                    category_stats[metric] = values
                elif isinstance(values, (list, np.ndarray)):
                    valid_values = [v for v in values if v >= 0]
                    if valid_values:
                        category_stats[metric] = {
                            'mean': float(np.mean(valid_values)),
                            'std': float(np.std(valid_values)),
                            'min': float(np.min(valid_values)),
                            'max': float(np.max(valid_values)),
                            'count': len(valid_values),
                            'total': len(values)
                        }

            if category_stats:
                stats[category] = category_stats

    return stats


def generate_evaluation_report(results_summary: Dict[str, Any],
                               game_name: str,
                               output_path: str = None) -> str:
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    report_lines = []

    # æŠ¥å‘Šæ ‡é¢˜
    report_lines.append(f"# {game_name.upper()} RAGç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š")
    report_lines.append("")

    # æ€»ä½“ç»Ÿè®¡
    total_segments = len(results_summary)
    successful_segments = sum(1 for r in results_summary.values() if 'error' not in r)
    total_data_count = sum(r.get('data_count', 0) for r in results_summary.values() if 'error' not in r)

    report_lines.append("## æ€»ä½“ç»Ÿè®¡")
    report_lines.append(f"- å¤„ç†åˆ†æ®µæ•°: {successful_segments}/{total_segments}")
    report_lines.append(f"- æ€»è¯„ä¼°æ•°æ®: {total_data_count} æ¡")
    report_lines.append(f"- æˆåŠŸç‡: {successful_segments/total_segments*100:.1f}%")
    report_lines.append("")

    # åˆ†æ®µè¯¦æƒ…
    report_lines.append("## åˆ†æ®µè¯¦æƒ…")
    for segment_id, summary in sorted(results_summary.items()):
        data_count = summary.get('data_count', 0)

        if 'error' in summary:
            report_lines.append(f"- åˆ†æ®µ {segment_id}: âŒ {summary['error']}")
        else:
            report_lines.append(f"- åˆ†æ®µ {segment_id}: âœ… {data_count} æ¡æ•°æ®")

    report_lines.append("")

    # æ”¶é›†æŒ‡æ ‡æ•°æ®
    retrieval_metrics = defaultdict(list)
    generation_metrics = defaultdict(list)

    for summary in results_summary.values():
        if 'error' not in summary:
            # æ”¶é›†æ£€ç´¢æŒ‡æ ‡
            retrieval_results = summary.get('retrieval_results', {})
            for metric, score in retrieval_results.items():
                if isinstance(score, (int, float)):
                    retrieval_metrics[metric].append(score)

            # æ”¶é›†ç”ŸæˆæŒ‡æ ‡
            generation_results = summary.get('generation_results', {})
            for metric, result in generation_results.items():
                if isinstance(result, dict) and 'mean' in result:
                    generation_metrics[metric].append(result['mean'])

    # æ£€ç´¢æŒ‡æ ‡æŠ¥å‘Š
    if retrieval_metrics:
        report_lines.append("## æ£€ç´¢æŒ‡æ ‡å¹³å‡å€¼")
        for metric, scores in retrieval_metrics.items():
            avg_score = np.mean(scores) if scores else 0.0
            report_lines.append(f"- {metric}: {avg_score:.4f} (æ¥è‡ª {len(scores)} ä¸ªåˆ†æ®µ)")
        report_lines.append("")

    # ç”ŸæˆæŒ‡æ ‡æŠ¥å‘Š
    if generation_metrics:
        report_lines.append("## ç”ŸæˆæŒ‡æ ‡å¹³å‡å€¼")
        for metric, scores in generation_metrics.items():
            avg_score = np.mean(scores) if scores else 0.0
            report_lines.append(f"- {metric}: {avg_score:.4f} (æ¥è‡ª {len(scores)} ä¸ªåˆ†æ®µ)")
        report_lines.append("")

    # åˆ†æ®µæ€§èƒ½å¯¹æ¯”
    if len(results_summary) > 1:
        report_lines.append("## åˆ†æ®µæ€§èƒ½å¯¹æ¯”")
        report_lines.append("### æ£€ç´¢æ€§èƒ½")
        for metric in ['precision', 'recall', 'f1', 'map']:
            if metric in retrieval_metrics:
                report_lines.append(f"#### {metric.upper()}")
                segment_scores = {}
                for segment_id, summary in results_summary.items():
                    if 'error' not in summary:
                        retrieval_results = summary.get('retrieval_results', {})
                        score = retrieval_results.get(metric, 0.0)
                        if isinstance(score, (int, float)):
                            segment_scores[segment_id] = score

                for segment_id, score in sorted(segment_scores.items()):
                    report_lines.append(f"- åˆ†æ®µ {segment_id}: {score:.4f}")
                report_lines.append("")

        report_lines.append("### ç”Ÿæˆæ€§èƒ½")
        for metric in ['accuracy', 'completeness', 'factuality', 'game_relevance']:
            if metric in generation_metrics:
                report_lines.append(f"#### {metric}")
                segment_scores = {}
                for segment_id, summary in results_summary.items():
                    if 'error' not in summary:
                        generation_results = summary.get('generation_results', {})
                        result = generation_results.get(metric, {})
                        if isinstance(result, dict) and 'mean' in result:
                            segment_scores[segment_id] = result['mean']

                for segment_id, score in sorted(segment_scores.items()):
                    report_lines.append(f"- åˆ†æ®µ {segment_id}: {score:.4f}")
                report_lines.append("")

    # ç”ŸæˆæŠ¥å‘Šæ–‡æœ¬
    report_text = "\n".join(report_lines)

    # ä¿å­˜æŠ¥å‘Š
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        print(f"ğŸ“Š è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

    return report_text


def clean_response_text(text: str) -> str:
    """æ¸…ç†å“åº”æ–‡æœ¬"""
    if not text:
        return ""

    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = " ".join(text.split())

    # ç§»é™¤å¯èƒ½çš„å‰ç¼€å’Œåç¼€
    prefixes_to_remove = [
        "å›ç­”ï¼š", "ç­”æ¡ˆï¼š", "Answer:", "Response:",
        "æ ¹æ®æä¾›çš„èµ„æ–™", "åŸºäºå‚è€ƒèµ„æ–™"
    ]

    for prefix in prefixes_to_remove:
        if text.startswith(prefix):
            text = text[len(prefix):].strip()

    return text


def validate_evaluation_data(rag_results: List[Dict], ground_truth: List[Dict]) -> bool:
    """éªŒè¯è¯„ä¼°æ•°æ®çš„å®Œæ•´æ€§"""
    if not rag_results:
        print("âŒ RAGç»“æœæ•°æ®ä¸ºç©º")
        return False

    if not ground_truth:
        print("âŒ çœŸå®æ ‡ç­¾æ•°æ®ä¸ºç©º")
        return False

    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_rag_fields = ['question', 'answer']
    required_gt_fields = ['question', 'answer']

    for i, item in enumerate(rag_results[:5]):  # åªæ£€æŸ¥å‰5æ¡
        for field in required_rag_fields:
            if field not in item:
                print(f"âŒ RAGç»“æœç¬¬{i+1}æ¡ç¼ºå°‘å­—æ®µ: {field}")
                return False

    for i, item in enumerate(ground_truth[:5]):  # åªæ£€æŸ¥å‰5æ¡
        for field in required_gt_fields:
            if field not in item:
                print(f"âŒ çœŸå®æ ‡ç­¾ç¬¬{i+1}æ¡ç¼ºå°‘å­—æ®µ: {field}")
                return False

    print(f"âœ… æ•°æ®éªŒè¯é€šè¿‡: RAGç»“æœ {len(rag_results)} æ¡, çœŸå®æ ‡ç­¾ {len(ground_truth)} æ¡")
    return True


def format_evaluation_summary(results: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–è¯„ä¼°ç»“æœæ‘˜è¦"""
    summary_lines = []

    if 'retrieval' in results:
        summary_lines.append("ğŸ“Š æ£€ç´¢è¯„ä¼°ç»“æœ:")
        for metric, score in results['retrieval'].items():
            if isinstance(score, (int, float)):
                summary_lines.append(f"  {metric}: {score:.4f}")

    if 'generation' in results:
        summary_lines.append("ğŸ“ ç”Ÿæˆè¯„ä¼°ç»“æœ:")
        for metric, result in results['generation'].items():
            if isinstance(result, dict) and 'mean' in result:
                summary_lines.append(f"  {metric}: {result['mean']:.4f} ({result['count']}/{result['total']})")

    return "\n".join(summary_lines)


def get_segment_info(game_name: str, segment_id: int, corpus_dir: str) -> Dict[str, Any]:
    """è·å–åˆ†æ®µä¿¡æ¯"""
    segment_info_file = Path(corpus_dir) / game_name / f"segment_{segment_id}" / "segment_info.json"

    if not segment_info_file.exists():
        return {
            'segment_id': segment_id,
            'start_date': 'unknown',
            'end_date': 'unknown',
            'document_count': 0
        }

    try:
        with open(segment_info_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸ åŠ è½½åˆ†æ®µ {segment_id} ä¿¡æ¯å¤±è´¥: {e}")
        return {
            'segment_id': segment_id,
            'start_date': 'unknown',
            'end_date': 'unknown',
            'document_count': 0
        }
