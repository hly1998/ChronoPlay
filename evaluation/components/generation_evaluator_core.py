# -*- coding: utf-8 -*-
"""
ç”Ÿæˆè¯„ä¼°æ ¸å¿ƒç»„ä»¶
"""

import json
import time
import numpy as np
from typing import List, Dict, Any
from tqdm import tqdm

from .game_evaluator import GameCorrectnessEvaluator


class GenerationEvaluatorCore:
    """ç”Ÿæˆè¯„ä¼°æ ¸å¿ƒ"""

    def __init__(self, config: Dict):
        self.config = config
        self.game_name = config.get('game_name', 'dyinglight2')

        # åˆå§‹åŒ–è¯„ä¼°å™¨
        self.evaluator = GameCorrectnessEvaluator(
            api_key=config['api_key'],
            base_url=config.get('openai_api', 'https://api.openai.com/v1'),
            model=config.get('model_name', 'gpt-4o'),
            temperature=config.get('temperature', 0.01)
        )

        self.default_metrics = ['correctness', 'faithfulness']

    def load_generation_results(self, filepath: str) -> List[Dict]:
        """åŠ è½½ç”Ÿæˆç»“æœ"""
        results = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line.strip())
                    results.append(self._prepare_item(item))
        return results

    def _prepare_item(self, item: Dict) -> Dict:
        """å‡†å¤‡è¯„ä¼°æ•°æ®é¡¹"""
        # æå–contexts
        contexts = item.get('contexts', [])
        if not contexts and item.get('retrieved_docs'):
            contexts = [
                doc.get('content', '') if isinstance(doc, dict) else str(doc)
                for doc in item['retrieved_docs']
            ]

        return {
            'question': item.get('question', ''),
            'rag_answer': item.get('answer', item.get('rag_answer', '')),
            'ground_truth_answer': item.get('ground_truth_answer', ''),
            'contexts': contexts,
            'question_index': item.get('question_index', -1)
        }

    def evaluate_metrics(
            self, data: List[Dict], metrics: List[str] = None,
            test_mode: bool = False) -> Dict[str, Any]:
        """è¯„ä¼°æŒ‡æ ‡"""
        if metrics is None:
            metrics = self.default_metrics

        if test_mode:
            data = data[:min(5, len(data))]
            print(f"ğŸ” æµ‹è¯•æ¨¡å¼ï¼šè¯„ä¼° {len(data)} ä¸ªæ ·æœ¬")

        detailed_results = []

        for i, item in enumerate(tqdm(data, desc="è¯„ä¼°ä¸­", unit="é¡¹")):
            question = item['question']
            rag_answer = item['rag_answer']
            ground_truth = item['ground_truth_answer']

            result = {'index': i, 'correctness_score': 0.0, 'faithfulness_score': 0.0}

            if not rag_answer or not ground_truth:
                detailed_results.append(result)
                continue

            if 'correctness' in metrics:
                result['correctness_score'] = self.evaluator.evaluate_single(
                    question, ground_truth, rag_answer
                )

            if 'faithfulness' in metrics and item['contexts']:
                result['faithfulness_score'] = self.evaluator.evaluate_faithfulness(
                    question, item['contexts'], rag_answer
                )

            detailed_results.append(result)
            time.sleep(0.1)

        return self._calculate_statistics(detailed_results, metrics)

    def _calculate_statistics(
            self, detailed_results: List[Dict], metrics: List[str]) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        overall = {}

        if 'correctness' in metrics:
            scores = [r['correctness_score'] for r in detailed_results]
            overall['correctness'] = {
                'mean': float(np.mean(scores)),
                'count': len(scores)
            }

        if 'faithfulness' in metrics:
            scores = [r['faithfulness_score'] for r in detailed_results]
            overall['faithfulness'] = {
                'mean': float(np.mean(scores)),
                'count': len(scores)
            }

        return {'overall': overall}

    def evaluate_basic_metrics(self, data: List[Dict]) -> Dict[str, Any]:
        """è¯„ä¼°åŸºç¡€æŒ‡æ ‡"""
        valid_answers = sum(1 for item in data if item.get('rag_answer', '').strip())
        total_length = sum(len(item.get('rag_answer', '')) for item in data if item.get('rag_answer'))

        return {
            'total_questions': len(data),
            'valid_answers': valid_answers,
            'answer_rate': valid_answers / len(data) if data else 0.0,
            'avg_answer_length': total_length / valid_answers if valid_answers > 0 else 0.0
        }


def print_results_summary(results: Dict):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    print("\n" + "=" * 50)
    print("è¯„ä¼°ç»“æœ")
    print("=" * 50)

    if 'llm_metrics' in results and 'overall' in results['llm_metrics']:
        overall = results['llm_metrics']['overall']
        if 'correctness' in overall:
            correctness = overall['correctness']
            print(f"\næ­£ç¡®æ€§: {correctness['mean']:.4f} (æ ·æœ¬æ•°: {correctness['count']})")
        if 'faithfulness' in overall:
            faithfulness = overall['faithfulness']
            print(f"å¿ å®åº¦: {faithfulness['mean']:.4f} (æ ·æœ¬æ•°: {faithfulness['count']})")

    if 'basic_metrics' in results:
        basic = results['basic_metrics']
        print(f"\næ€»é—®é¢˜æ•°: {basic.get('total_questions', 0)}")
        print(f"æœ‰æ•ˆç­”æ¡ˆæ•°: {basic.get('valid_answers', 0)}")
        print(f"ç­”æ¡ˆç‡: {basic.get('answer_rate', 0):.4f}")
