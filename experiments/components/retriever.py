# -*- coding: utf-8 -*-
"""
å‘é‡æ£€ç´¢å™¨ç»„ä»¶
"""

from pathlib import Path
from typing import List, Dict, Any
import numpy as np
from tqdm import tqdm
import time

from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import (
    VectorStoreIndex,
    Document,
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.core.vector_stores.types import VectorStoreQuery
from openai import OpenAI

from .config import RAGConfig, RetrievalConfig
from .embedding_service import create_embedding_service


class VectorRetriever:
    """ç®€åŒ–çš„å‘é‡æ£€ç´¢å™¨"""

    def __init__(self, config):
        self.config = config
        self.index = None
        self.retriever = None

        # æ”¯æŒä¸¤ç§é…ç½®ç±»å‹
        if isinstance(config, RetrievalConfig):
            self._init_with_retrieval_config(config)
        else:
            self._init_with_rag_config(config)

    def _init_with_retrieval_config(self, config: RetrievalConfig):
        """ä½¿ç”¨RetrievalConfigåˆå§‹åŒ–"""
        # åˆ›å»ºåµŒå…¥æœåŠ¡
        self.embedding_service = create_embedding_service(config)

        # é…ç½®LlamaIndexçš„åµŒå…¥æ¨¡å‹ - ç»Ÿä¸€ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
        Settings.embed_model = OpenAIEmbedding(
            api_key=config.api_key,
            api_base=config.base_url,
            model=config.embedding_model
        )

    def _init_with_rag_config(self, config: RAGConfig):
        """ä½¿ç”¨RAGConfigåˆå§‹åŒ–ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        # é…ç½®åµŒå…¥æ¨¡å‹
        Settings.embed_model = OpenAIEmbedding(
            api_key=config.api_key,
            api_base=config.base_url,
            model=config.embedding_model
        )

        # ç”¨äºæ‰¹é‡embeddingsçš„OpenAIå®¢æˆ·ç«¯
        self.openai_client = OpenAI(
            api_key=config.api_key,
            base_url=config.base_url
        )

    def build_index(self, documents: List[Document]) -> bool:
        """æ„å»ºæˆ–åŠ è½½å‘é‡ç´¢å¼•"""
        # ç”Ÿæˆç´¢å¼•è·¯å¾„
        index_name = self._get_index_name()
        index_path = Path(self.config.index_dir) / self.config.game_name / index_name

        # å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
        if index_path.exists() and not self.config.force_rebuild:
            if self._load_existing_index(index_path):
                return True

        # æ„å»ºæ–°ç´¢å¼•
        return self._build_new_index(documents, index_path)

    def _get_index_name(self) -> str:
        """ç”Ÿæˆç´¢å¼•åç§°"""
        model_name = self.config.embedding_model.replace('-', '_').replace('/', '_')

        if self.config.target_segment_id:
            suffix = "_with_timeless" if self.config.include_timeless else ""
            return f"{model_name}/segment_{self.config.target_segment_id}{suffix}"
        return f"{model_name}/full_corpus"

    def _load_existing_index(self, index_path: Path) -> bool:
        """åŠ è½½å·²æœ‰ç´¢å¼•"""
        try:
            if self.config.verbose:
                print(f"ğŸ“‚ åŠ è½½å·²æœ‰ç´¢å¼•: {index_path.name}")

            storage_context = StorageContext.from_defaults(persist_dir=str(index_path))
            self.index = load_index_from_storage(storage_context)
            self.retriever = self.index.as_retriever(similarity_top_k=self.config.top_k)

            if self.config.verbose:
                print("âœ… ç´¢å¼•åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            if self.config.verbose:
                print(f"âš ï¸ ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
            return False

    def _build_new_index(self, documents: List[Document], index_path: Path) -> bool:
        """æ„å»ºæ–°ç´¢å¼•"""
        try:
            if self.config.verbose:
                print(f"ğŸ”¨ æ„å»ºæ–°ç´¢å¼•ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")

            self.index = VectorStoreIndex.from_documents(documents, show_progress=self.config.verbose)

            # ä¿å­˜ç´¢å¼•
            index_path.mkdir(parents=True, exist_ok=True)
            self.index.storage_context.persist(persist_dir=str(index_path))

            self.retriever = self.index.as_retriever(similarity_top_k=self.config.top_k)

            if self.config.verbose:
                print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {index_path}")
            return True
        except Exception as e:
            if self.config.verbose:
                print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            return False

    def retrieve(self, query: str) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.retriever:
            raise ValueError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–")

        try:
            nodes = self.retriever.retrieve(query)
            return [{
                'id': node.id_,
                'content': node.get_content(),
                'metadata': node.metadata,
                'score': getattr(node, 'score', 0.0)
            } for node in nodes]
        except Exception as e:
            if self.config.verbose:
                print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return []

    def batch_get_embeddings(self, texts: List[str], batch_size: int = 100) -> np.ndarray:
        """
        æ‰¹é‡è·å–æ–‡æœ¬çš„embeddingå‘é‡

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°

        Returns:
            embeddingçŸ©é˜µ
        """
        # å¦‚æœæ˜¯RetrievalConfigï¼Œä½¿ç”¨æ–°çš„åµŒå…¥æœåŠ¡
        if isinstance(self.config, RetrievalConfig) and hasattr(self, 'embedding_service'):
            return self.embedding_service.get_embeddings(texts, batch_size)

        # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰çš„OpenAIå®¢æˆ·ç«¯
        embeddings = []

        progress_bar = tqdm(range(0, len(texts), batch_size), desc="Embedding", disable=not self.config.verbose)

        for i in progress_bar:
            batch_texts = texts[i:i + batch_size]

            try:
                response = self.openai_client.embeddings.create(
                    model=self.config.embedding_model,
                    input=batch_texts
                )

                batch_embeddings = [item.embedding for item in response.data]
                embeddings.extend(batch_embeddings)

                # æ·»åŠ å»¶æ—¶é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                time.sleep(0.05)

            except Exception as e:
                if self.config.verbose:
                    print(f"âŒ è·å–embeddingå¤±è´¥: {e}")
                # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
                embeddings.extend([[0.0] * 1536] * len(batch_texts))

        return np.array(embeddings)

    def batch_retrieve(self, queries: List[str]) -> List[List[Dict[str, Any]]]:
        """
        æ‰¹é‡æ£€ç´¢ç›¸å…³æ–‡æ¡£

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨

        Returns:
            æ¯ä¸ªæŸ¥è¯¢å¯¹åº”çš„æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not self.index:
            raise ValueError("ç´¢å¼•æœªåˆå§‹åŒ–")

        try:
            # æ‰¹é‡è·å–æŸ¥è¯¢çš„embeddings
            query_embeddings = self.batch_get_embeddings(queries)

            # æ‰§è¡Œæ‰¹é‡æ£€ç´¢
            all_results = []

            for i, (query, query_embedding) in enumerate(zip(queries, query_embeddings)):

                # ç›´æ¥ä½¿ç”¨å·²è®¡ç®—çš„embeddingå‘é‡è¿›è¡Œæ£€ç´¢ï¼Œé¿å…é‡å¤è®¡ç®—
                try:
                    # ç›´æ¥ä½¿ç”¨vector_storeæŸ¥è¯¢ï¼Œä¼ å…¥å·²è®¡ç®—çš„embedding
                    vector_store = self.index.vector_store

                    # åˆ›å»ºVectorStoreQueryå¯¹è±¡
                    query_obj = VectorStoreQuery(
                        query_embedding=query_embedding.tolist(),
                        similarity_top_k=self.config.top_k
                    )

                    # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
                    query_result = vector_store.query(query_obj)

                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    results = []
                    if hasattr(query_result, 'nodes') and query_result.nodes:
                        for node in query_result.nodes:
                            results.append({
                                'id': node.id_,
                                'content': node.get_content(),
                                'metadata': node.metadata,
                                'score': getattr(node, 'score', 0.0)
                            })
                    elif hasattr(query_result, 'similarities') and query_result.similarities:
                        # å¦‚æœæ²¡æœ‰nodesï¼Œä½¿ç”¨similaritieså’Œids
                        for j, (node_id, similarity) in enumerate(zip(query_result.ids, query_result.similarities)):
                            if j >= self.config.top_k:
                                break
                            # ä»docstoreè·å–èŠ‚ç‚¹ä¿¡æ¯
                            try:
                                node = self.index.docstore.get_node(node_id)
                                results.append({
                                    'id': node_id,
                                    'content': node.get_content(),
                                    'metadata': node.metadata,
                                    'score': float(similarity)
                                })
                            except Exception:
                                # å¦‚æœæ— æ³•è·å–èŠ‚ç‚¹è¯¦æƒ…ï¼Œè‡³å°‘è¿”å›åŸºæœ¬ä¿¡æ¯
                                results.append({
                                    'id': node_id,
                                    'content': f"Document {node_id}",
                                    'metadata': {},
                                    'score': float(similarity)
                                })

                    all_results.append(results)

                except Exception:
                    # è¿”å›ç©ºç»“æœï¼Œé¿å…è§¦å‘å•ç‹¬çš„APIè°ƒç”¨
                    all_results.append([])

            return all_results

        except Exception as e:
            if self.config.verbose:
                print(f"âŒ æ‰¹é‡æ£€ç´¢å¤±è´¥ï¼Œå›é€€åˆ°å•ä¸ªæ£€ç´¢: {e}")
            # å›é€€åˆ°å•ä¸ªæ£€ç´¢
            return [self.retrieve(query) for query in queries]
