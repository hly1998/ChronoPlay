# -*- coding: utf-8 -*-
"""
ChronoPlayæ£€ç´¢è¯„ä¼°å™¨
ä¸“é—¨ç”¨äºè¯„ä¼°RAGç³»ç»Ÿçš„æ£€ç´¢æ•ˆæœ - ä»…åŸºç¡€æŒ‡æ ‡
"""

import json
import os
import argparse
import numpy as np
from tqdm import tqdm
from collections import defaultdict
from typing import List, Dict, Any
from pathlib import Path

from components.retrieval_evaluator_core import RetrievalEvaluator


class ChronoPlayRetrievalEvaluator:
    """ChronoPlayæ£€ç´¢è¯„ä¼°å™¨ - ä»…åŸºç¡€æŒ‡æ ‡è¯„ä¼°"""

    def __init__(self, config: Dict):
        self.config = config
        self.game_name = config.get('game_name', 'dyinglight2')
        self.target_segment_id = config.get('target_segment_id', None)
        self.evaluator = RetrievalEvaluator(k_values=[1, 3, 5])

    def load_retrieval_results(self, retrieval_results_path: str) -> List[Dict]:
        """åŠ è½½æ£€ç´¢ç»“æœæ•°æ®"""
        retrieval_data = []
        with open(retrieval_results_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue

                item = json.loads(line.strip())
                original_qa_data = item.get('original_qa_data', {})

                # æå– ground truth æ–‡æ¡£IDs
                ground_truth_docs = original_qa_data.get('retrieved_docs', [])
                ground_truth_doc_ids = [
                    doc['metadata']['id'] for doc in ground_truth_docs
                    if isinstance(doc, dict) and 'metadata' in doc and 'id' in doc['metadata']
                ]

                # æå–æ£€ç´¢ç»“æœæ–‡æ¡£IDs
                retrieved_docs = item.get('retrieved_docs', [])
                retrieved_doc_ids = [
                    doc['metadata']['id'] for doc in retrieved_docs
                    if isinstance(doc, dict) and 'metadata' in doc and 'id' in doc['metadata']
                ]

                # æå–æ¨¡å‹ä¿¡æ¯
                retrieval_config = item.get('retrieval_config', {})
                retrieval_method = retrieval_config.get('retrieval_method', 'vector')
                embedding_model = retrieval_config.get('embedding_model', 'unknown')
                top_k = retrieval_config.get('top_k', 5)

                # ç”Ÿæˆæ¨¡å‹æ ‡è¯†
                model_key = f"bm25_k{top_k}" if retrieval_method == 'bm25' else f"{embedding_model}_k{top_k}"

                eval_item = {
                    'question': item.get('question', ''),
                    'retrieved_docs': retrieved_docs,
                    'retrieved_doc_ids': retrieved_doc_ids,
                    'ground_truth_doc_ids': ground_truth_doc_ids,
                    'ground_truth_docs': ground_truth_docs,
                    'retrieval_time': item.get('retrieval_time', 0),
                    'config': retrieval_config,
                    'metadata': {
                        'question_type': original_qa_data.get('question_type', 'unknown'),
                        'difficulty': original_qa_data.get('difficulty', 'unknown'),
                        'segment_id': retrieval_config.get('segment_id'),
                        'k': top_k,
                        'retrieval_method': retrieval_method,
                        'embedding_model': embedding_model if retrieval_method != 'bm25' else "BM25",
                        'embedding_service': retrieval_config.get('embedding_service', 'unknown'),
                        'model_key': model_key
                    }
                }
                retrieval_data.append(eval_item)

        return retrieval_data

    def evaluate_retrieval_metrics(self, retrieval_data: List[Dict]) -> Dict[str, Any]:
        """è¯„ä¼°æ£€ç´¢æŒ‡æ ‡ï¼šRecall@K, F1@K, NDCG@K (k=1,3,5)"""
        return self.evaluator.evaluate_metrics(retrieval_data)

    def analyze_performance(self, retrieval_data: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææ£€ç´¢æ€§èƒ½ï¼ˆæŒ‰ä¸åŒç»´åº¦åˆ†ç»„ï¼‰"""
        return self.evaluator.analyze_performance(retrieval_data)

    def evaluate_segments(self, retrieval_results_dir: str, output_dir: str = None,
                          segment_ids: List[int] = None) -> Dict[str, Any]:
        """å¯¹æŒ‡å®šåˆ†æ®µè¿›è¡Œæ£€ç´¢è¯„ä¼° - æŒ‰æ¨¡å‹åˆ†ç»„"""
        # ç¡®å®šè¦è¯„ä¼°çš„åˆ†æ®µ
        if segment_ids is None:
            # è‡ªåŠ¨å‘ç°å¯ç”¨çš„åˆ†æ®µ
            results_path = Path(retrieval_results_dir)
            segment_ids = []

            # æ£€ç´¢ç»“æœæ–‡ä»¶æ¨¡å¼
            patterns = [
                f"retrieval_{self.game_name}_segment_*_*.jsonl"
            ]

            for pattern in patterns:
                for file in results_path.glob(pattern):
                    try:
                        # ä»æ–‡ä»¶åæå–åˆ†æ®µID
                        parts = file.stem.split('_')
                        for i, part in enumerate(parts):
                            if part == 'segment' and i + 1 < len(parts):
                                segment_id = int(parts[i + 1])
                                if segment_id not in segment_ids:
                                    segment_ids.append(segment_id)
                                break
                    except (ValueError, IndexError):
                        continue
            segment_ids.sort()

        if self.target_segment_id:
            segment_ids = [self.target_segment_id]

        # æŒ‰æ¨¡å‹åˆ†ç»„çš„ç»“æœ
        model_results = {}
        all_segments_summary = {}

        print(f"ğŸ” è¯„ä¼° {len(segment_ids)} ä¸ªåˆ†æ®µ: {segment_ids}")

        for segment_id in tqdm(segment_ids, desc="è¯„ä¼°åˆ†æ®µ"):
            # æŸ¥æ‰¾æ£€ç´¢ç»“æœæ–‡ä»¶
            retrieval_files = list(Path(retrieval_results_dir).glob(
                f"retrieval_{self.game_name}_segment_{segment_id}_*.jsonl"))

            if not retrieval_files:
                all_segments_summary[segment_id] = {
                    'segment_id': segment_id,
                    'error': 'No retrieval results file found'
                }
                continue

            # æŒ‰æ¨¡å‹åˆ†ç»„å¤„ç†æ–‡ä»¶
            segment_model_results = {}

            for retrieval_file in retrieval_files:
                # åŠ è½½åˆ†æ®µæ•°æ®
                try:
                    retrieval_data = self.load_retrieval_results(str(retrieval_file))
                    if not retrieval_data:
                        continue

                    # æŒ‰æ¨¡å‹åˆ†ç»„æ•°æ®
                    model_groups = defaultdict(list)
                    for item in retrieval_data:
                        model_key = item['metadata']['model_key']
                        model_groups[model_key].append(item)

                    # å¯¹æ¯ä¸ªæ¨¡å‹ç»„è¿›è¡Œè¯„ä¼°
                    for model_key, model_data in model_groups.items():

                        # æ‰§è¡Œæ£€ç´¢è¯„ä¼°
                        results = {}
                        results['metrics'] = self.evaluate_retrieval_metrics(model_data)
                        results['analysis'] = self.analyze_performance(model_data)
                        results['model_info'] = {
                            'retrieval_method': model_data[0]['metadata']['retrieval_method'],
                            'embedding_model': model_data[0]['metadata']['embedding_model'],
                            'embedding_service': model_data[0]['metadata']['embedding_service'],
                            'top_k': model_data[0]['metadata']['k'],
                            'model_key': model_key
                        }

                        # ä¿å­˜æ¨¡å‹ç»“æœ
                        if output_dir:
                            model_output_dir = Path(output_dir) / model_key
                            model_output_dir.mkdir(parents=True, exist_ok=True)
                            output_file = model_output_dir / \
                                f"{self.game_name}_segment_{segment_id}_retrieval_evaluation.json"
                            with open(output_file, 'w', encoding='utf-8') as f:
                                json.dump(results, f, ensure_ascii=False, indent=2)

                        # è®°å½•æ¨¡å‹ç»“æœ
                        if model_key not in model_results:
                            model_results[model_key] = {}

                        model_results[model_key][segment_id] = {
                            'segment_id': segment_id,
                            'data_count': len(model_data),
                            'metrics': results.get('metrics', {}),
                            'analysis': results.get('analysis', {}),
                            'model_info': results.get('model_info', {}),
                            'output_file': str(output_file) if output_dir else None
                        }

                        segment_model_results[model_key] = {
                            'data_count': len(model_data),
                            'metrics': results.get('metrics', {}),
                            'model_info': results.get('model_info', {})
                        }

                except Exception:
                    pass

            # è®°å½•åˆ†æ®µæ€»ç»“
            if segment_model_results:
                all_segments_summary[segment_id] = {
                    'segment_id': segment_id,
                    'models': segment_model_results,
                    'total_models': len(segment_model_results)
                }
            else:
                all_segments_summary[segment_id] = {
                    'segment_id': segment_id,
                    'error': 'No valid retrieval data found'
                }

        # ä¿å­˜æ¯ä¸ªæ¨¡å‹çš„æ€»ç»“æŠ¥å‘Š
        if output_dir:
            for model_key, model_data in model_results.items():
                model_summary_file = Path(output_dir) / model_key / \
                    f"{self.game_name}_retrieval_evaluation_summary.json"
                model_summary_file.parent.mkdir(parents=True, exist_ok=True)
                with open(model_summary_file, 'w', encoding='utf-8') as f:
                    json.dump(model_data, f, ensure_ascii=False, indent=2)

            # ä¿å­˜æ€»ä½“æ€»ç»“æŠ¥å‘Š
            overall_summary_file = Path(output_dir) / f"{self.game_name}_retrieval_evaluation_overall_summary.json"
            with open(overall_summary_file, 'w', encoding='utf-8') as f:
                json.dump(all_segments_summary, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

        # æ‰“å°æ€»ç»“
        self._print_model_summary(model_results, all_segments_summary)

        return model_results

    def _print_model_summary(self, model_results: Dict, all_segments_summary: Dict):
        """æ‰“å°æŒ‰æ¨¡å‹åˆ†ç»„çš„æ£€ç´¢è¯„ä¼°æ€»ç»“"""
        print("\n" + "=" * 80)
        print(f"{self.game_name.upper()} æ£€ç´¢è¯„ä¼°æ€»ç»“")
        print("=" * 80)

        total_segments = len(all_segments_summary)
        successful_segments = len([s for s in all_segments_summary.values() if 'error' not in s])
        print(f"\nğŸ“Š è¯„ä¼°äº† {len(model_results)} ä¸ªæ¨¡å‹, {successful_segments}/{total_segments} ä¸ªåˆ†æ®µ")

        # æŒ‰æ¨¡å‹æ‰“å°ç»“æœ
        for model_key, model_data in model_results.items():
            print(f"\nğŸ”§ {model_key}")

            # æ±‡æ€»æŒ‡æ ‡
            model_metrics = defaultdict(list)
            total_data = 0
            for segment_data in model_data.values():
                if 'error' not in segment_data:
                    total_data += segment_data.get('data_count', 0)
                    metrics = segment_data.get('metrics', {})
                    for metric, score in metrics.items():
                        if isinstance(score, (int, float)):
                            model_metrics[metric].append(score)

            print(f"   æ•°æ®é‡: {total_data} | åˆ†æ®µ: {len(model_data)}")

            # æ‰“å°å¹³å‡æŒ‡æ ‡
            for k in [1, 3, 5]:
                metrics_str = []
                for metric_type in ['recall', 'f1', 'ndcg']:
                    key = f'{metric_type}_at_{k}'
                    if key in model_metrics and model_metrics[key]:
                        avg = np.mean(model_metrics[key])
                        metrics_str.append(f"{metric_type.upper()}@{k}={avg:.3f}")
                if metrics_str:
                    print(f"   K={k}: {' | '.join(metrics_str)}")

        print("\nâœ… è¯„ä¼°å®Œæˆ")

    def run_single_evaluation(self, retrieval_results_path: str, output_path: str = None) -> Dict:
        """è¿è¡Œå•æ–‡ä»¶æ£€ç´¢è¯„ä¼°"""
        retrieval_data = self.load_retrieval_results(retrieval_results_path)
        if not retrieval_data:
            return {}

        results = {
            'metrics': self.evaluate_retrieval_metrics(retrieval_data),
            'analysis': self.analyze_performance(retrieval_data)
        }

        # ä¿å­˜ç»“æœ
        if output_path:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")

        # æ‰“å°æ€»ç»“
        self._print_single_summary(results)
        return results

    def _print_single_summary(self, results: Dict):
        """æ‰“å°å•æ–‡ä»¶è¯„ä¼°ç»“æœæ€»ç»“"""
        print("\n" + "=" * 60)
        print("æ£€ç´¢è¯„ä¼°ç»“æœ")
        print("=" * 60)

        metrics = results.get('metrics', {})
        print(f"\nğŸ“Š è¯„ä¼°äº† {metrics.get('valid_queries', 0)}/{metrics.get('total_queries', 0)} ä¸ªæŸ¥è¯¢")

        for k in [1, 3, 5]:
            metrics_str = []
            for metric_type in ['recall', 'f1', 'ndcg']:
                key = f'{metric_type}_at_{k}'
                if key in metrics:
                    metrics_str.append(f"{metric_type.upper()}@{k}={metrics[key]:.3f}")
            if metrics_str:
                print(f"   K={k}: {' | '.join(metrics_str)}")

        timing = results.get('analysis', {}).get('timing_analysis', {})
        if timing.get('total_samples', 0) > 0:
            print(f"\nâ±ï¸  å¹³å‡æ£€ç´¢æ—¶é—´: {timing['avg_retrieval_time']:.3f}s")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ChronoPlayæ£€ç´¢æ•ˆæœè¯„ä¼°',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è‡ªåŠ¨è¯„ä¼°æ‰€æœ‰åˆ†æ®µçš„æ£€ç´¢æ•ˆæœ
  python retrieval_evaluator.py

  # è¯„ä¼°ç‰¹å®šåˆ†æ®µ
  python retrieval_evaluator.py --segment_id 1

  # è¯„ä¼°ç‰¹å®šæ¸¸æˆçš„æ‰€æœ‰åˆ†æ®µ
  python retrieval_evaluator.py --game dyinglight2

  # å•æ–‡ä»¶è¯„ä¼°
  python retrieval_evaluator.py --retrieval_results ./results/retrieval_dyinglight2_segment_1_*.jsonl
        """
    )

    # åŸºç¡€é…ç½®
    parser.add_argument('--game', type=str, default='dyinglight2',
                        help='æ¸¸æˆåç§° (é»˜è®¤: dyinglight2)')
    parser.add_argument('--segment_id', type=int,
                        help='ç›®æ ‡åˆ†æ®µID (ä¸æŒ‡å®šåˆ™è¯„ä¼°æ‰€æœ‰å¯ç”¨åˆ†æ®µ)')

    # è¾“å…¥è¾“å‡ºé…ç½®
    parser.add_argument('--retrieval_results', type=str,
                        help='å•ä¸ªæ£€ç´¢ç»“æœæ–‡ä»¶è·¯å¾„ (å¦‚æœæŒ‡å®šåˆ™è¿›å…¥å•æ–‡ä»¶æ¨¡å¼)')
    parser.add_argument('--results_dir', type=str,
                        default='evaluation/retrieval_results',
                        help='æ£€ç´¢ç»“æœç›®å½• (é»˜è®¤: ./retrieval_results)')
    parser.add_argument('--output_dir', type=str,
                        default='evaluation/retrieval_evaluation',
                        help='è¯„ä¼°ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: ./retrieval_evaluation)')

    args = parser.parse_args()

    print(f"ğŸ” ChronoPlayæ£€ç´¢è¯„ä¼°å™¨ - {args.game}")

    if args.retrieval_results:
        mode = "single_file"
        retrieval_file = args.retrieval_results
        if not os.path.exists(retrieval_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {retrieval_file}")
            return
    else:
        mode = "batch"
        results_dir = args.results_dir
        if not os.path.exists(results_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {results_dir}")
            return

    # åˆ›å»ºè¯„ä¼°å™¨
    config = {
        'game_name': args.game,
        'target_segment_id': args.segment_id
    }
    evaluator = ChronoPlayRetrievalEvaluator(config)
    os.makedirs(args.output_dir, exist_ok=True)

    try:
        if mode == "single_file":
            output_file = os.path.join(args.output_dir, f"{args.game}_single_retrieval_evaluation.json")
            evaluator.run_single_evaluation(retrieval_file, output_file)
        else:
            evaluator.evaluate_segments(results_dir, args.output_dir)

    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")


if __name__ == '__main__':
    main()
